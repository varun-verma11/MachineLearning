%{
    Input arguments:
        EXAMPLES - matrix of given set of examples
        ATTRIBUTES - attributes for the given set of examples
        BINARY_TARGETS - specifies the emotion label for corresponding
            examples (for which the tree is to be built)

    Output argument:
        tree - a decision tree generated by the given examples
%}
function [ tree ] = desicion_tree_learning( examples, attributes, binary_targets )
    if (length(unique(binary_targets)) == 1)
        tree = struct('op',[],'kids',[],'class', binary_targets(1));
    elseif (isempty(attributes))
        tree = struct('op',[],'kids',[],'class', majority_value(binary_targets));
    else
        % select the best AU corresponding to greatest information gain
        best = choose_best_attribute(attributes, examples, binary_targets);
        subtree = cell(1, 2);  % left/right subtree
        for i = 0:1 % each possible value v_i of best
            % split and classify examples and targets accordingly
            [ new_examples, new_targets ] = split_examples_for_attribute(examples, binary_targets, best, i);
            if (isempty(new_examples))
                subtree{i+1} = struct('op',best,'kids',[],'class', majority_value(binary_targets));
            else
                sub_attrs = attributes(attributes ~= best);
                subtree{i+1} = DecisionTreeLearning(new_examples, sub_attrs, new_targets);
            end
        end
        tree = struct('op',best,'kids',[],'class',[]);
        tree.kids = subtree;
    end
end


%{
    This function is used to return the best attribute from the given
    ttributes in the given examples, If multiple attributes which are
    equally good are found then the first one is returned.
    
    Input:
        ATTRIBUTES - set of attributes to compare
        EXAMPLES - set of examples for which these attributes have to be
            compared
        TARGETS - emotion label for the corresponding tree
    Output:
        BEST_ATTRIBUTE - return the attribute which has the highest
            information gain
%}
function [ best_attribute ] = choose_best_attribute( attributes, examples, targets )
    information_gains = arrayfun(@(attr) information_gain(attr, examples, targets), attributes);
    [~, index] = max(information_gains);
    best_attribute = attributes(index);
end


%{
    This function return the value which appears the most in the given set
    of values
%}
function [ value ] = majority_value(values)
    value = mode(double(values));
end


% splits examples and targets according to column corresponding to best AU
function [ ret_examples, ret_bin_targets ] = split_examples_for_attribute(examples, targets, best, selector)
    new_examples = examples;
    new_targets = targets;
    last_row_index = 0;

    [num_rows, ~] = size(examples);
    for i = 1:num_rows
        if (examples(i,best) == selector)
            last_row_index = last_row_index + 1;
            new_targets(last_row_index) = targets(i);
            new_examples(last_row_index,:) = examples(i,:);
        end
    end

    if (last_row_index ~= 0)
        ret_examples = new_examples(1:last_row_index,:);
        ret_bin_targets  = new_targets(1:last_row_index);
    else
        ret_examples = [];
        ret_bin_targets = [];
    end

end

%{
    Input:
        au       - specifies attribute to calculate ig attribute
        examples - given set of examples
        targets  - specifies the au's for the emotion 
    Output:
        gain     - value of information gain for this attribute
%}
function [ gain ] = information_gain( action_unit, examples, targets )
    % implementation of formula from lecture slides
    [ entropy_all_set, ~]    = entropy(0 ,  0, examples, targets);
    [ e_negative, prob_negative ] = entropy(action_unit , 0, examples, targets);
    [ e_positive, prob_positive ] = entropy(action_unit , 1, examples, targets);
    gain = entropy_all_set - prob_negative * e_negative - prob_positive * e_positive;
end

% calculates the entropy for the given attribute for the given data
function [ entropy, probability ] = entropy( au, set, examples, targets )

    assert (min(targets) >= 0 && max(targets) <= 1);

    pos = 0;
    neg = 0;
    prob_pos = 0;
    prob_neg = 0;

    [num_rows, ~] = size(examples);
    for i = 1:num_rows
        if ((au == 0 || examples(i,au) == set) && targets(i) == 1)
            pos = pos + 1;
        elseif ((au == 0 || examples(i,au) == set) && targets(i) == 0)
            neg = neg + 1;
        end
    end

    total       = pos + neg;
    probability = total/num_rows;

    if (total ~= 0)
        prob_pos = pos/total;
        prob_neg = neg/total;
    end

    entropy = abs(-prob_pos * ml_log2(prob_pos)) ...
        + abs(-prob_neg * ml_log2(prob_neg));
    end

% custom wrapper around log2 to set log2(0) = 0 instead of NaN
function [ out ] = ml_log2(in)
    if (in == 0)
        out = 0;
    else
        out = log2(in);
end
end


